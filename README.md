# Product Classification

In this test we ask you to build a model to classify products into their categories according to their features.



## Index
- [Dataset Description](#dataset-description)
- [Task description](#task-description)
- [Structure](#structure)
- [Modules API Documentation](#modules-api-documentation)
- [Try the Inference API](#try-the-inference-api)
- [Running the project](#running-the-project)
    - [ETL Pipeline](#etl-pipeline)
    - [Training Pipeline](#training-pipeline)
    - [Running the training pipeline with Docker](#running-the-training-pipeline-with-docker)
    - [Inference API](#inference-api)
    - [API Documentation](#api-documentation)



## Dataset Description

The dataset is a simplified version of [Amazon 2018](https://jmcauley.ucsd.edu/data/amazon/), only containing products and their descriptions.

The dataset consists of a jsonl file where each is a json string describing a product.

Example of a product in the dataset:
```json
{
 "also_buy": ["B071WSK6R8", "B006K8N5WQ", "B01ASDJLX0", "B00658TPYI"],
 "also_view": [],
 "asin": "B00N31IGPO",
 "brand": "Speed Dealer Customs",
 "category": ["Automotive", "Replacement Parts", "Shocks, Struts & Suspension", "Tie Rod Ends & Parts", "Tie Rod Ends"],
 "description": ["Universal heim joint tie rod weld in tube adapter bung. Made in the USA by Speed Dealer Customs. Tube adapter measurements are as in the title, please contact us about any questions you 
may have."],
 "feature": ["Completely CNC machined 1045 Steel", "Single RH Tube Adapter", "Thread: 3/4-16", "O.D.: 1-1/4", "Fits 1-1/4\" tube with .120\" wall thickness"],
 "image": [],
 "price": "",
 "title": "3/4-16 RH Weld In Threaded Heim Joint Tube Adapter Bung for 1-1/4&quot; Dia by .120 Wall Tube",
 "main_cat": "Automotive"
}
```

### Field description
- also_buy/also_view: IDs of related products
- asin: ID of the product
- brand: brand of the product
- category: list of categories the product belong to, usually in hierarchical order
- description: description of the product
- feature: bullet point format features of the product
- image: url of product images (migth be empty)
- price: price in US dollars (might be empty)
- title: name of the product
- main_cat: main category of the product

`main_cat` can have one of the following values:
```json
["All Electronics",
 "Amazon Fashion",
 "Amazon Home",
 "Arts, Crafts & Sewing",
 "Automotive",
 "Books",
 "Buy a Kindle",
 "Camera & Photo",
 "Cell Phones & Accessories",
 "Computers",
 "Digital Music",
 "Grocery",
 "Health & Personal Care",
 "Home Audio & Theater",
 "Industrial & Scientific",
 "Movies & TV",
 "Musical Instruments",
 "Office Products",
 "Pet Supplies",
 "Sports & Outdoors",
 "Tools & Home Improvement",
 "Toys & Games",
 "Video Games"]
```

[Download dataset](https://drive.google.com/file/d/1Zf0Kdby-FHLdNXatMP0AD2nY0h-cjas3/view?usp=sharing)

Data can be read directly from the gzip file as:
```python
def parse(path):
    g = gzip.open(path, 'r')
    for l in g:
        yield json.loads(l)
```


## Task description

- You should create a model that predicts `main_cat` using any of the other fields except `category` list. The model should be developed in python, you can use any pre-trained models and thirdparty 
libraries you need (for example huggingface).

- You should create a HTTP API endpoint that is capable of performing inference and return the predicted `main_cat` when receiving the rest of product fields.

- Both the training code (if needed) and the inference API should be dockerized and easy for us to run and test locally. **Only docker build and docker run commands should be necessary to perform training or setting up the inference API**.

- You should also provide a detailed analysis of the performance of your model. **In this test we're not looking for the best model performance but we expect a good understanding of your solution performance and it's limitations**.

- We will value:
    - Code cleanliness and good practices.
    - API design and architecture.
    - A clear understanding of the model performance, strengths and weak points.

- Answer the following questions:
	- What would you change in your solution if you needed to predict all the categories?
    - How would you deploy this API on the cloud?
	- If this model was deployed to categorize products without any supervision which metrics would you check to detect data drifting? When would you need to retrain?




## Structure 
- [`docs/`](./docs/) API documentation of the project autogenerated using pdoc.
- [`notebooks/`](./notebooks/) Jupyter notebooks used for benchmarking and exploratory data analysis.
- [`src/`](./src/) contains all the modules code for the product classifier.
- [`src/data_pipeline/`](./src/data_pipeline/) contains the module for the ETL pipeline.
- [`src/training/`](./src/training/) contains the training module.
- [`src/inference`](./src/training/) contains the inference API module.



## Modules API Documentation

The API documentation can be found [here](./docs/index.html).
Alternatively it can be served locally by running the following command in an environment with pdoc installed.
```bash
pdoc  src
```

## Try the Inference API
 
 The inference API can be tested by accessing the Swagger UI at http://localhost:8000/docs after starting the API. This endpoint also shows the documentation of the API. There is also alternative documentation available http://localhost:8000/redoc.

## Running the project

The project consist of three different components which can run independently. The components are the ETL pipeline, training module and the inference API.

### ETL Pipeline

The ETL can use either PySpark or Pandas for processing the data in a distributed or in memory stile, respectively.
After loading the data, it preprocesses it and loads it into a parquet file for training.

The ETL pipeline can be run by executing the following command in the root directory of the project:
```bash
python -m src.etl.etl_pipeline <input_file> <output_file> <tokenizer_name>
```
The `tokenizer_name` argument is optional and defaults to `None` if you do not want to save the data encoded.

An example of running the ETL pipeline is shown below.
```bash
python -m src.etl.etl_pipeline ./data/raw/amz_products_small.jsonl.gz "./data/processed/amz_products_small_processed_bert_tiny_v1.parquet/" "prajjwal1/bert-tiny"

```

### Training Pipeline

The training pipeline uses the preprocessed data to train a BERT model on PyTorch for the product classification task. The model is trained using the Hugging Face `transformers` library.

The training pipeline can be run by executing the following command in the root directory of the project.
```bash
python -m src.etl.etl_pipeline --config <config_file> 
```

An example of running the training pipeline is shown below.
```bash
python -m src.training.training_pipeline --config src/training/configs/bert_tiny_config.yml --data_fraction=0.03
```

Any additional arguments can be passed to the training pipeline as well. The training pipeline can be run with the following arguments:
- config_file (str): Path to the configuration file.
-  input_data_path (str): Path to the input JSONL.gz file.
- input_data_encoded (bool): Whether the data is already encoded. Defaults to False.
- bert_model_name (str): Name of the BERT model to use for tokenization. Defaults to 'bert-base-uncased'.
- data_load ('memory' | 'distributed'): Method to load the data. Defaults to 'distributed'.
- trainable_layers (int | None): Number of layers to keep trainable. None for all layers. Defaults to None.
- num_epochs (int): Number of epochs to train the model. Defaults to 3.
- batch_size (int): Batch size for training and validation. Defaults to 8.
- test_batch_size (int): Batch size for testing. Defaults to 32.
- learning_rate (float): Learning rate for the optimizer. Defaults to 1e-5.
- optimizer (str): Optimizer to use for training. Defaults to 'AdamW'.
- seed (int): Random seed for reproducibility. Defaults to 42.
- device (str): Device to use for training. Defaults to 'cuda'.
- data_fraction (float): Fraction of the data to sample for training. Defaults to 1.0.

These are the default values for the configuration.
```yaml
input_data_path: 'data/preprocessed/amz_products_small_preprocessed_v1.parquet'
input_data_encoded: False
bert_model_name: 'bert-base-uncased'
data_load: 'distributed'
trainable_layers: None
num_epochs: 3
batch_size: 8
test_batch_size: 32
learning_rate: 1e-5
optimizer: 'AdamW'
device: 'cuda'
seed: 42
data_fraction: 1.0
```


#### Running the training pipeline with Docker

The training pipeline Docker image can be built and run using the following commands from the root directory of the project.
```bash
# Build the Docker image
docker build --build-arg DATABRICKS_PASSWORD=<password> -t fever_training -f src/training/Dockerfile .
# Run the Docker container
docker run --rm --gpus all fever_training --config <config_file> 
```

The run command can be modified to include additional arguments as well to override the configuration file values.

> ⚠️ Bare in mind that the `--gpus all` flag only works if you have a NVIDIA GPU and have installed the NVIDIA Container Toolkit. If you don't have a GPU, you can remove the flag and run the container on the CPU.


### Inference API

The inference API uses the trained model to make predictions on new data. The API is built using FastAPI and Pydantic for the data validation.

The inference API can be run by executing the following command in the root directory of the project.
```bash
python -m src.inference.app 
```

The API can be accessed at http://localhost:8000/docs for Swagger UI and http://localhost:8000/redoc for Redoc documentation.

#### Running the inference API with Docker

The inference API Docker image can be built and run using the following commands from the root directory of the project.
```bash
# Build the Docker image
docker build --build-arg DATABRICKS_PASSWORD=<password> -t fever_inference -f src/inference/Dockerfile .
# Run the Docker container
docker run --rm -p 8000:8000 fever_inference -it
```
